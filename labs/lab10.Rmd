---
title: "Lab 10"
author: "Nabila Ahmed"
output: pdf_document
date: "11:59PM May 11, 2020"
---


In the first part of this lab, we will be joining three datasets in an effort to make a design matrix that predicts if a bill will be paid on time. Load up the three files:

```{r}
pacman::p_load(tidyverse, magrittr, data.table, R.utils)
bills = fread("https://github.com/kapelner/QC_Math_390.4_Spring_2020/raw/master/labs/bills_dataset/bills.csv.bz2")
payments = fread("https://github.com/kapelner/QC_Math_390.4_Spring_2020/raw/master/labs/bills_dataset/payments.csv.bz2")
discounts = fread("https://github.com/kapelner/QC_Math_390.4_Spring_2020/raw/master/labs/bills_dataset/discounts.csv.bz2")
```

The unit we care about is the bill. The metric we care about is "paid in full" and it's binary. We would like to build the best design matrix we can and of course generate the y. Warning: this data is highly anonymized and there is likely zero signal! So don't expect to get predictive accuracy. The value of the exercise is in the practice. I think this may be one of the most useful exercises in the entire semester.

I will create the basic steps for you guys. First, join the three datasets in an intelligent way. You will need to examine the datasets beforehand.

```{r}
head(bills)
setnames(bills, "amount", "tot_amount")
setnames(payments, "amount", "paid_amount")
head(payments)
head(discounts)
bills_with_payments = merge(bills, payments, by.x ="id", by.y = "bill_id", all.x = TRUE)
bills_with_payments[, id.y := NULL]
bills_with_payments

bills_payments_discounts = merge(bills_with_payments, discounts, by.x = "discount_id", by.y = "id", all.x = TRUE)
bills_payments_discounts
```

Now create the response metric "paid_in_full" and create the design matrix by ensuring the unit is bill. How should you featurize? Should you create some features? What type(s) should they be? 

```{r}
bills_data = bills_payments_discounts %>% 
  group_by(id) %>% 
  summarise(total_paid_amount = sum(paid_amount), customer_id = first(customer_id), discount_id = first(discount_id), total_amount = first(tot_amount)) %>%
  mutate(total_paid_amount = ifelse(is.na(total_paid_amount), 0, total_paid_amount), paid_in_full = ifelse(total_paid_amount >= total_amount, 1, 0))
table(bills_data$paid_in_full, useNA = "always") 
```

Fit a tree to this data. Try to use `YARF` if you have it. If not, use the package `rpart`. Below is a guide to installing `YARF` and ensuring it works.

First, ensure you have the Java JDK installed. The JDK is NOT the JRE. The former allows you to compile Java programs and the latter allows you only to run Java programs. Then insure that `rJava` is installed and working. In other words, the following should work and give the same output from practice lecture 12. If it doesn't, try the code that is commented out to reinstall. Google errors. Frustration in libraries and platforms not working on your computer is unfortunately part of computer science and thus part of data science.

```{r}
options(java.parameters = "-Xmx4000m")
pacman::p_load(rJava)
#if that doesn't work, use:
# install.packages("rJava", type = "source")
# library(rJava)
.jinit() #this initializes the JVM in the background and if this runs with no issues nor output, you probably have rJava installed and connected to the JDK properly.
java_double = .jnew("java/lang/Double", 3.1415)
java_double
class(java_double)
.jclass(java_double)
#call an instance method 
.jcall(java_double, "I", "intValue") #java_double.intValue();
#call a static method
J("java/lang/String", "valueOf", java_double) #String.valueOf(java_double);
J("java/lang/String", "valueOf", x) #some sort of alphanumeric code for the pointer address
```

It is important to have rJava working on your computer as a fair number of R packages really do make use of it. It's a good thing to have in your toolbox in general.

Now ensure that YARF is installed properly:


```{r}
# pacman::p_install_gh("kapelner/YARF/YARFJARs", ref = "dev")
# pacman::p_install_gh("kapelner/YARF/YARF", ref = "dev")
pacman::p_load(YARF)
```

If that printed out "YARF can now make use of [n] cores", you are in business.

Now create a training-test split and make the tree model and provide oos performance metrics: create a confusion table and compute FDR and FOR.

```{r}
set.seed(1)
train_size = 5000
train_indices = sample(1 : nrow(bills_data), train_size)
bill_train = bills_data[train_indices, ]
y_train = bill_train$paid_in_full
X_train = bill_train
X_train$total_amount 

test_size = 5000
test_indices = sample(setdiff(1 : nrow(bills_data), train_indices), test_size)
bill_test = bills_data[test_indices, ]
y_test = bill_test$paid_in_full
X_test = bill_test
bill_test$total_amount

tree_model = YARF(X=X_train, y= y_train)

yhat_train = predict(tree_model, X_train)
yhat_test = predict(tree_model, X_test)

oos_conf_table = table(y_test, yhat_test)
oos_conf_table


n = sum(oos_conf_table)
fp = oos_conf_table[1, 2]
fn = oos_conf_table[2, 1]
tp = oos_conf_table[2, 2]
tn = oos_conf_table[1, 1]
num_pred_pos = sum(oos_conf_table[, 2])
num_pred_neg = sum(oos_conf_table[, 1])
num_pos = sum(oos_conf_table[2, ])
num_neg = sum(oos_conf_table[1, ])
precision = tp / num_pred_pos

FDR = fp/num_pred_pos
FDR

FOR = fn/num_pred_neg
FOR
```


We are done with this unit. 

Let's take a look at the simulated sine curve data from practice lecture 12. Below is the code for the data generating process:

```{r}
rm(list = ls())
n = 500
sigma = 0.3
x_min = 0
x_max = 10
x = runif(n, x_min, x_max)
f_x = function(x){sin(x)}
y = f_x(x) + rnorm(n, 0, sigma)
```

Plot an example dataset of size 500:

```{r}
pacman::p_load(ggplot2)
ggplot()+ geom_point(aes(x=x,y=y))
```

Locate the optimal node size hyperparameter for the regression tree model.

```{r} 
#TO-DO
```

Plot the regression tree model with the optimal node size.

```{r}
#TO-DO
```

Provide the bias-variance decomposition of this DGP fit with this model. It is a lot of code, but it is in the practice lectures.

```{r}
#TO-DO
```

Load the boston housing data. Leave 25% of the observations oos for honest validation. 

```{r}
library(MASS)
data(Boston)
Boston = Boston[sample(1:nrow(Boston)), ]
Boston_train = Boston[1 : 380, ]
Boston_test = Boston[381 : nrow(Boston), ]
```

Fit a linear model with all first-order interactions and provide std err of residuals in the test set.

```{r}
mod = lm(medv ~ .*., Boston_train)
mod
yhat = predict(mod, Boston_test)
sd(Boston_test$medv - yhat)
```

Bag this algorithm with $M = 1000$ and provide std err of residuals in the test set. 

```{r}
M = 1000
for (m in 1:M) {
  
}
```

What is your gain over the unbagged model? Why is there a gain?

#TO-DO












