---
title: "Lab 5"
author: "Nabila Ahmed"
output: pdf_document
date: "11:59PM March 7, 2020"
---

Load the Boston housing data frame and create the vector $y$ (the median value) and matrix $X$ (all other features) from the data frame. Name the columns the same as Boston except for the first name it "(Intercept)".

```{r}
y = MASS::Boston$medv
X = MASS::Boston[ , 1: 13] 

```

Run the OLS linear model to get $b$, the vector of coefficients. Do not use `lm`.

```{r}
X = cbind(1,as.matrix(X))
b = solve(t(X) %*% X ) %*% t(X) %*% y

```

Find the hat matrix for this regression `H`. Verify its dimension is correct and verify its rank is correct.

```{r}
H = X %*% solve(t(X) %*% X ) %*% t(X)
dim(H)
pacman::p_load(Matrix)
rankMatrix(H)
```

Verify this is a projection matrix by verifying the two sufficient conditions. Use the `testthat` library's `expect_equal(matrix1, matrix2, tolerance = 1e-2)`.

```{r}
pacman::p_load(testthat)
expect_equal(H, t(H))
expect_equal(H, H %*% H)

```

Find the matrix that projects onto the space of residuals `Hcomp` and find its rank. Is this rank expected?

```{r}
Hcomp = diag(nrow(X)) - H
rankMatrix(H)
rankMatrix(Hcomp, tol = 1e-2)
```

Verify this is a projection matrix by verifying the two sufficient conditions. Use the `testthat` library.

```{r}
expect_equal(Hcomp, t(Hcomp))
expect_equal(Hcomp, Hcomp %*% Hcomp)
```

Use `diag` to find the trace of both `H` and `Hcomp`.

```{r}
sum(diag(H))
sum(diag(Hcomp))
```

Do you have a conjecture about the trace of an orthogonal projection matrix?
 
trace is equal to the rank

Find the eigendecomposition of both `H` and `Hcomp` as `eigenvals_H`, `eigenvecs_H`, `eigenvals_Hcomp`, `eigenvecs_Hcomp`. Verify these results are the correct dimensions.

```{r}
eigen_H = eigen(H)
eigen_Hcomp = eigen(Hcomp)

eigenvals_H = eigen_H$values
eigenvecs_H = eigen_H$vectors
eigenvals_Hcomp = eigen_Hcomp$values
eigenvecs_Hcomp = eigen_Hcomp$vectors

length(eigenvals_H)
dim(eigenvecs_H)
length(eigenvals_Hcomp)
dim(eigenvecs_Hcomp)
```

The eigendecomposition suffers from numerical error which is making them become imaginary. We can coerce imaginary numbers back to real by using the `Re` function. There is also lots of numerical error. Use the `Re` function to coerce to real and the `round` function to round all four objects to the nearest 10 digits.

```{r, warning = FALSE, message = FALSE}
eigenvals_H = round(as.numeric(eigenvals_H), 10)
eigenvecs_H = round(Re(eigenvecs_H), 10)
eigenvals_Hcomp = round(as.numeric(eigenvals_Hcomp), 10)
eigenvecs_Hcomp = round(Re(eigenvecs_Hcomp), 10)
```

Print out the eigenvalues of both `H` and `Hcomp`. Is this expected?

```{r}
eigenvals_H
eigenvecs_H
eigenvals_Hcomp
```

Find the length of all eigenvectors of `H` in one line. 

```{r}
apply(eigenvecs_H, MARGIN =2, FUN = function(v){
  sqrt(sum(v^2))
})
```

Is this expected? What is the convention for eigenvectors in R's `eigen` function?

Yes. The convention is length 1.

The first p+1 eigenvectors are the columns of $X$ but they are in arbitrary order. Find the column that represents the one-vector. 

```{r}
head(eigenvecs_H[, 3])
```

Why is it not exactly 506 1's?

Numeric error

Use the first p+1 eigenvectors as a model matrix and run the OLS model of medv on that model matrix. 


```{r}
mod1 = lm(y ~ X)
mod2 = lm(y ~ eigenvecs_H[, 1:14])
summary(mod1)
summary(mod2)
```

Is b about the same above (in arbitrary order)?

NO, the eigen vectors are scaled to be unit length

Calculate $\hat{y}$ using the hat matrix.

```{r}
y_hat= H %*% y
y_hat
```

Calculate $e$ two ways: (1) the difference of $y$ and $\hat{y}$ and (2) the projection onto the space of the residuals. Verify the two means of calculating the residuals provide the same results via `expect_equal`.

```{r}
e1 = y -y_hat
e2 = Hcomp %*% y
expect_equal(e1, e2)
```

Calculate $R^2$ using the angle relationship between the responses and their predictions.

```{r}

length_of_vec = function(v){sqrt(sum(v^2))}
y_avg_adj = y - mean(y)
y_yhat_adj = y_hat - mean(y)
(sum(y_avg_adj * y_yhat_adj) / (length_of_vec(y_avg_adj) * length_of_vec(y_yhat_adj))) ** 2

```

Find the cosine-squared of $y - \bar{y}$ and $\hat{y} - \bar{y}$ and verify it is the same as $R^2$.

```{r}
summary(mod1)$r.squared
```

Verify $\hat{y}$ and $e$ are orthogonal.

```{r}
sum(y_hat*e1)

```

Verify $\hat{y} - \bar{y}$ and $e$ are orthogonal.

```{r}
sum((y_hat -mean(y)) *e1)
```

Verify the sum of squares identity which we learned was due to the Pythagorean Theorem (applies since the projection is specifically orthogonal). You need to compute all three quantities first: SST, SSR and SSE.

```{r}
e1 = y -y_hat
SST= sum((y-mean(y))^2)
SST
SSR = sum((y_hat - mean(y))^2)
SSR
SSE = sum(e1^2)
SSE


```

Create a matrix that is $(p + 1) \times (p + 1)$ full of NA's. Label the columns the same columns as X. Do not label the rows. For the first row, find the OLS estimate of the $y$ regressed on the first column only and put that in the first entry. For the second row, find the OLS estimates of the $y$ regressed on the first and second columns of $X$ only and put them in the first and second entries. For the third row, find the OLS estimates of the $y$ regressed on the first, second and third columns of $X$ only and put them in the first, second and third entries, etc. For the last row, fill it with the full OLS estimates.

```{r}
newMatrix = matrix(data = NA,nrow = nrow(X), ncol = ncol(X))
colnames(newMatrix)=colnames(X) #Label the columns the same columns as X. 
for(j in 1:ncol(newMatrix)){
  xj=X[,1:j,drop=FALSE]
  b = solve(t(xj) %*% xj) %*% t(xj) %*% y
  newMatrix[j,1:j]=b
}
newMatrix
```

Examine this matrix. Why are the estimates changing from row to row as you add in more predictors?
The estimates changing from row to row as we are adding more predictors because more weights are being added.

Clear the workspace and load the diamonds dataset in the package `ggplot2`.

```{r}
rm(list = ls()) #removing objects/ clearing workspace
pacman::p_load(ggplot2)
library(ggplot2)
diamonds

```

Extract $y$, the price variable and `col`, the nominal variable "color" as vectors.

```{r}
y= diamonds$price
col = diamonds$color
```

Convert the `col` vector to $X$ which contains an intercept and an appropriate number of dummies. Let the color G be the refernce category as it is the modal color. Name the columns of $X$ appropriately. The first should be "(Intercept)". Delete `col`.

```{r}

X = model.matrix(price~col,diamonds)
colnames(X)= c("Intercept","D","E","F","H","I","J")
X
```

Repeat the iterative exercise above we did for Boston here.

```{r}
newMatrix = matrix(data = NA,nrow = nrow(X), ncol = ncol(X))
colnames(newMatrix)=colnames(X) #Label the columns the same columns as X. 
for(j in 1:ncol(newMatrix)){
  xj=X[,1:j,drop=FALSE]
  b = solve(t(xj) %*% xj) %*% t(xj) %*% y
  newMatrix[j,1:j]=b
}
newMatrix
```

Why didn't the estimates change as we added more and more features?
The estimates didn't chance because we were just adding more weights 
#TO-DO

Model `price` with both `color` and `clarity` with and without an intercept and report the coefficients.

```{r}
x1=model.matrix(price~col + clarity, diamonds) # with intercept
x1
x2= model.matrix(price~0 + col+clarity, diamonds) # without intercept
x2
```

Which coefficients did not change between the models and why?
The clarity coefficients did not change between the models because the G was held as the reference category and G only effects the color coefficients.



Create a 2x2 matrix with the first column 1's and the next column iid normals. Find the absolute value of the angle (in degrees, not radians) between the two columns.

```{r}
X = matrix(NA,nrow=2,ncol=2)
X[,1]=1
X[,2] = rnorm(2)
X

angle = function(x,y){
  dotprod = x%*%y
  normX= norm(x,type = "2")
  normY= norm(y,type ="2")
  theta = acos(dotprod/ (normX * normY))
  as.numeric(theta)
}
abs(angle(X[,1],X[,2]))
```

Repeat this exercise $Nsim = 1e5$ times and report the average absolute angle.

```{r}
Nsim = 1e5
X = matrix(NA,nrow=2,ncol = 2)
X[,1]=1
X[,2]=rnorm(2)
rep(X,Nsim)

sumAngle = 0
for(i in 1:Nsim){
  X[,1]=1
  X[,2]=rnorm(2)
  sumAngle=sumAngle+ abs(angle(X[,1],X[,2]))
}

avgAngle = sumAngle/Nsim
avgAngle
```

Create a 2xn matrix with the first column 1's and the next column iid normals. Find the absolute value of the angle (in degrees, not radians) between the two columns. For $n \in {10, 50, 100, 200, 500, 1000}$, report the average absolute angle over $Nsim = 1e5$ simulations.

```{r}

#array = c(10, 50, 100, 200, 500, 1000)
#for(k in array){
#  theta = 0
#  for(i in 1 : Nsim){
#  iidnorms = c(rnorm(n))
#  B = as.matrix(cbind(1, iidnorms), nrow =k , ncol = 2)
#  theta <- theta + acos( sum(B[,1]*B[,2]) / ( sqrt(sum(B[,1] * B[,1])) * sqrt(sum(B[,2] * B[,2])) ) )
#  }
#  avgAngle = theta / Nsim
#  avgAngle
#}

n = 10
theta = 0
for(i in 1 : Nsim){
iidnorms = c(rnorm(n))
B = as.matrix(cbind(1, iidnorms), nrow = n, ncol = 2)
theta <- theta + acos( sum(B[,1]*B[,2]) / ( sqrt(sum(B[,1] * B[,1])) * sqrt(sum(B[,2] * B[,2])) ) )
}
avgAngle = theta / Nsim
avgAngle

n = 50
theta = 0
for(i in 1 : Nsim){
iidnorms = c(rnorm(n))
B = as.matrix(cbind(1, iidnorms), nrow = n, ncol = 2)
theta <- theta + acos( sum(B[,1]*B[,2]) / ( sqrt(sum(B[,1] * B[,1])) * sqrt(sum(B[,2] * B[,2])) ) )
}
avgAngle = theta / Nsim
avgAngle

n = 100
theta = 0
for(i in 1 : Nsim){
iidnorms = c(rnorm(n))
B = as.matrix(cbind(1, iidnorms), nrow = n, ncol = 2)
theta <- theta + acos( sum(B[,1]*B[,2]) / ( sqrt(sum(B[,1] * B[,1])) * sqrt(sum(B[,2] * B[,2])) ) )
}
avgAngle = theta / Nsim
avgAngle

n = 200
theta = 0
for(i in 1 : Nsim){
iidnorms = c(rnorm(n))
B = as.matrix(cbind(1, iidnorms), nrow = n, ncol = 2)
theta <- theta + acos( sum(B[,1]*B[,2]) / ( sqrt(sum(B[,1] * B[,1])) * sqrt(sum(B[,2] * B[,2])) ) )
}
avgAngle = theta / Nsim
avgAngle

n = 500
theta = 0
for(i in 1 : Nsim){
iidnorms = c(rnorm(n))
B = as.matrix(cbind(1, iidnorms), nrow = n, ncol = 2)
theta <- theta + acos( sum(B[,1]*B[,2]) / ( sqrt(sum(B[,1] * B[,1])) * sqrt(sum(B[,2] * B[,2])) ) )
}
avgAngle = theta/Nsim
avgAngle

n = 1000
theta = 0
for(i in 1 : Nsim){
iidnorms = c(rnorm(n))
B = as.matrix(cbind(1, iidnorms), nrow = n, ncol = 2)
theta <- theta + acos( sum(B[,1]*B[,2]) / ( sqrt(sum(B[,1] * B[,1])) * sqrt(sum(B[,2] * B[,2])) ) )
}
avgAngle = theta/Nsim
avgAngle
```

What is this absolute angle converging to? Why does this make sense?
This absolute angle converges to 1.57.It makes sense because running the code for 2 by n matrix where n is upto 1000, therefore it should converge the value.

Create a vector $y$ by simulating $n = 100$ standard iid normals. Create a matrix of size 100 x 2 and populate the first column by all ones (for the intercept) and the second column by 100 standard iid normals. Find the $R^2$ of an OLS regression of `y ~ X`. Use matrix algebra.

```{r}
n = 100
y = rnorm(n)
X = matrix(NA, n, 2)
X[ , 1] = 1
X[ , 2] = y
X
Xt = t(X) 
XtX = Xt %*% X
XtXinv = solve(XtX)
b = XtXinv %*% Xt %*% y
b
yhat = X %*% b
e = y - yhat
Rsq = (var(y) - var(e)) / var(y)
Rsq

amod = lm(y ~ X)
summary(amod)$r.squared
```

Write a for loop to each time bind a new column of 100 standard iid normals to the matrix $X$ and find the $R^2$ each time until the number of columns is 100. Create a vector to save all $R^2$'s. What happened??

```{r}
X = as.data.frame(X)
R2_vec = c()
for(j in ncol(X) : 100){
  X[ , j] = rnorm(nrow(X))
  R2_vec = c(R2_vec, summary(lm('as.matrix(y)~as.matrix(X)'))$r.squared)
}
X
R2_vec
```

Add one final column to $X$ to bring the number of columns to 101. Then try to compute $R^2$. What happens?

```{r}
X = X[ , ncol(X) + 1]
amod = lm(y ~ X)
summary(amod)$r.squared

```

